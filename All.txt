https://colab.research.google.com/drive/19tW0KjoG-DpOjKhfafmauF0xN8QOTNrL?usp=sharing
-------------------------------------------------------------------------------------------------------------------
#fuzzy logic operation
A = dict()
B = dict()

 
A = {"a": 0.2, "b": 0.3, "c": 0.6, "d": 0.6}
B = {"a": 0.9, "b": 0.9, "c": 0.4, "d": 0.5}
 
print('The First Fuzzy Set is :', A)
print('The Second Fuzzy Set is :', B)
 
def fuzzy_union(A,B): 
 Y = dict()
 for A_key, B_key in zip(A, B):
    A_value = A[A_key]
    B_value = B[B_key]
 
    if A_value > B_value:
        Y[A_key] = A_value
    else:
        Y[B_key] = B_value
 print('Fuzzy Set Union is :', Y)


def fuzzy_intersection(A,B): 
 Y = dict()
 for A_key, B_key in zip(A, B):
    A_value = A[A_key]
    B_value = B[B_key]
 
    if A_value < B_value:
        Y[A_key] = A_value
    else:
        Y[B_key] = B_value
 print('Fuzzy Set intersection is :', Y)

def fuzzy_complement(A): 
 Y = dict()
 for A_key in A:
    A_value = A[A_key]
    Y[A_key] = 1 - A_value 
 print('Fuzzy Set complement of set A is :', Y)

def fuzzy_difference(A,B): 
 Y = dict()
 for A_key, B_key in zip(A, B):
    A_value = A[A_key]
    B_value = B[B_key]
    B_value = 1-B_value
 
    if A_value < B_value:
        Y[A_key] = A_value
    else:
        Y[B_key] = B_value
 print('Fuzzy Set difference is :', Y)

fuzzy_union(A,B)
fuzzy_intersection(A,B)
fuzzy_complement(A)
fuzzy_difference(A,B)

-------------------------------------------------------------------------------------------------------------------

# genetic algo string

import random

# Define problem
def count_ones(binary_string):
    return sum(1 for bit in binary_string if bit == '1')

# Define genetic algorithm
def genetic_algorithm(population_size, mutation_rate, max_generations):
    # Generate initial population
    population = [''.join(random.choices(['0', '1'], k=8)) for _ in range(population_size)]

    for generation in range(max_generations):
        # Evaluate fitness of each candidate solution
        fitness_scores = [count_ones(candidate) for candidate in population]

        # Check if satisfactory solution is found
        if 8 in fitness_scores:
            fittest_individual = population[fitness_scores.index(8)]
            print(f"Found optimal solution: {fittest_individual}")
            return fittest_individual

        # Select fittest individuals
        fittest_individuals = [x for _,x in sorted(zip(fitness_scores,population), reverse=True)][:int(0.2*population_size)]

        # Create new individuals
        new_population = []
        while len(new_population) < population_size:
            parent1, parent2 = random.choices(fittest_individuals, k=2)
            offspring = parent1[:4] + parent2[4:]
            offspring = ''.join([mutate(bit, mutation_rate) for bit in offspring])
            new_population.append(offspring)

        # Replace population with new generation
        population = new_population

        # Print the fittest individual in the population
        print(f"Generation {generation}: {fittest_individuals[0]}, fitness={fitness_scores[0]}")

    # If we reach here, we have not found an optimal solution within the maximum number of generations
    print("Could not find optimal solution")
    return None

# Define mutation operator
def mutate(bit, mutation_rate):
    if random.random() < mutation_rate:
        return '0' if bit == '1' else '1'
    else:
        return bit

# Run genetic algorithm
genetic_algorithm(population_size=100, mutation_rate=0.1, max_generations=1000)



-------------------------------------------------------------------------------------------------------------------

#knapsack using ga
import random
 
def generate_population(size):
  population = []
  for _ in range(size):
    genes = [0,1]
    chromosome = []
    for _ in range(len(items)):
      chromosome.append(random.choice(genes))

    population.append(chromosome)
  return population 

def calculate_fitness(chromosome):
  total_weight = 0
  total_value = 0
  for i in range(len(chromosome)):
    if chromosome[i] == 1:
      total_weight += items[i][0]
      total_value += items[i][1]
  if total_weight > max_weight:
    return 0
  else:
    return total_value  

def select_chromosome(population): 
 fitness_values = []
 for chromosome in population:
   fitness_values.append(calculate_fitness(chromosome))

 fitness_values = [float(i)/sum(fitness_values) for i in fitness_values]

 parent1 = random.choices(population,weights=fitness_values, k=1)[0]
 parent2 = random.choices(population,weights=fitness_values, k=1)[0]

 return parent1,parent2

def crossover(parent1,parent2):
  crossover_point = random.randint(0,len(items)-1)
  child1 = parent1[0:crossover_point] + parent2[crossover_point:]
  child2 = parent2[0:crossover_point] + parent1[crossover_point:]

  return child1, child2

def mutate(chromosome):
  mutation_point = random.randint(0,len(items)-1)
  if chromosome[mutation_point] == 0:
    chromosome[mutation_point] = 1
  else:
    chromosome[mutation_point] = 0

  return chromosome

def get_best(population):
  fitness_values = []
  for chromosome in population:
    fitness_values.append(calculate_fitness(chromosome))

  max_value = max(fitness_values)
  max_index = fitness_values.index(max_value)
  return population[max_index]


items = [[1,2],[2,4],[3,4],[4,5],[5,7],[6,9],[7,8],[8,10]]

print('Available items : \n' , items)

max_weight = 10
population_size = 20
mutation_prob = 0.2
generations = 10

print("\nGenetic algorithm parameters:")

print("Max weight:", max_weight)
print("Population:", population_size)
print("Mutation probability:", mutation_prob)
print("Generations:", generations, "\n")
print("Performing genetic evolution:")

population = generate_population(population_size)


for _ in range(generations):
  parentl, parent2 = select_chromosome(population)
  childl, child2 = crossover(parentl, parent2)

  if random.uniform(0, 1) < mutation_prob:
    childl = mutate(childl)

  if random.uniform(0, 1) < mutation_prob:
    child2 = mutate(child2)

  population = [childl, child2] + population[2:]


best = get_best(population)

selected=[]

total_weight = 0
total_value = 0
for i in range(len(best)):
 if best[i]==1:
   total_weight+=items[i][0]
   total_value+=items[i][1]
   selected.append(i)

print("\nThe best solution:")
print("Selected item:" , selected)
print("Weight:", total_weight)
print("value:", total_value)




-------------------------------------------------------------------------------------------------------------------

#perceptron 
from numpy import array, random, dot

from random import choice

from matplotlib import pyplot as pit

step_function = lambda x: 0 if x < 0 else 1

training_dataset = [(array([0,0,1]), 0), (array([0,1,1]), 1),(array([1,0,1]), 0), (array([1,1,1]), 1),]
weights = random.rand(3)

error = []

learning_rate = 0.2

n=10

for j in range(n):

 x, expected = choice(training_dataset)

 result = dot(weights, x)

 err = expected-step_function(result)

 error.append(err) 

 weights += learning_rate * err * x

for x, _ in training_dataset:

 result = dot(x, weights)

 print('{}: {} => {}'.format(x[:2], result, step_function(result)))



-------------------------------------------------------------------------------------------------------------------

#adaline implementation

import numpy as np

X = np.array([[1,1,1],[1,2,3],[2,3,4],[1,2,4],[5,2,3]])
Y = np.array([1,0,1,0,1])

#print(X.shape[1])

w = np.zeros(X.shape[1])
b = 0
learning_rate = 0.5

def activation_function(X,w,b):
  return np.sum(w * X) + b

def predict(X,w,b):
  weighted_sum = activation_function(X,w,b)
  if weighted_sum >= 2:
    return 1
  else:
    return 0

for i in range(100):
  te = 0
  for j in range(len(X)):
    x = X[j]
    target = Y[j]
    prediction = predict(x,w,b)
    err = target - prediction

    w += learning_rate * err * x
    b += learning_rate * err     

    te += (err * err)
  print('least mean square eroor in epoch : ', i , ' = ', te)    


print('w : ', w , ' bias : ', b)
  


-------------------------------------------------------------------------------------------------------------------

#back propagation using or gate

import numpy as np

x1=np.array([0,0,1,1])
x2=np.array([0,1,0,1])
y_true=np.array([0,1,1,1])
w=[0.5,0.3,0.6,0.2,0.1,0.7]

b=[0.1,0.3,0.4]
rate=0.5

def sigmoid_numpy(X):
 return 1/(1+np.exp(-X))

def prediction_function(x1,x2,coefl,coef2,intercept):
 weighted_sum = coefl*x1 + coef2*x2 + intercept
 return sigmoid_numpy(weighted_sum)

def log_loss(y_true, y_predicted):

 epsilon = 1e-15

 y_predicted_new = [max(i,epsilon) for i in y_predicted]
 y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
 y_predicted_new = np.array(y_predicted_new)
 return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))

def gradient_descent(x1,x2,w,y,epochs):
  n = len(x1)
  for i in range(epochs):
    h1 = prediction_function(x1,x2,w[0],w[1],b[0])
    h2 = prediction_function(x1,x2,w[2],w[3],b[1])
    y_predicted = prediction_function(h1,h2,w[4],w[5],b[2])
    loss = log_loss(y_true,y_predicted)

    w0d = (1/n)*np.dot(np.transpose(x1), (y_predicted-y_true))
    w1d = (1/n)*np.dot(np.transpose(x1), (y_predicted-y_true))
    w2d = (1/n)*np.dot(np.transpose(x2), (y_predicted-y_true))
    w3d = (1/n)*np.dot(np.transpose(x2), (y_predicted-y_true))
    w4d = (1/n)*np.dot(np.transpose(h1), (y_predicted-y_true))
    w5d = (1/n)*np.dot(np.transpose(h2), (y_predicted-y_true))

    bias_d = np.mean(y_predicted-y_true)
    w[0] = w[0] - rate * w0d
    w[1] = w[1] - rate * w1d
    w[2] = w[2] - rate * w2d
    w[3] = w[3] - rate * w3d
    w[4] = w[4] - rate * w4d
    w[5] = w[5] - rate * w5d

    b[0] = b[0] - rate * bias_d
    b[1] = b[1] - rate * bias_d
    b[2] = b[2] - rate * bias_d

    print(f'Epoch:{i}, y_predicted:{y_predicted}, y_true:{y}, loss:{loss}')
  
  return w[0], w[1], w[2],w[3] ,w[4], w[5]

gradient_descent(x1,x2,w,y_true,20)

-------------------------------------------------------------------------------------------------------------------
#XOR function implementation using McCulloch Pits Neuron:
import numpy as np
X=[ [0,0], [0,1], [1,0], [1,1] ]
W=np.array([1,1])
X=np.array(X)
def predict(x,w):
 sum=np.sum(w*x)
 if sum==1:
  return 1
 else:
  return 0
for e in X:
 y=predict(e,W)
 print(y)


-------------------------------------------------------------------------------------------------------------------

# Activation Fucntion
import numpy as np
import matplotlib .pyplot as plt
def curve(x):
 return 4*x+5
x=np.arange(0,20)
y=curve(x)
print(x)
print(y)
plt.scatter(x,y)
plt.plot(x,y)
plt.show()

-------------------------------------------------------------------------------------------------------------------

#binary step
import numpy as np
import matplotlib.pyplot as plt

X = np.arange(-50,50)
def binary_step(x):
  th = 0.1
  if x >= th:
    return 1
  else:
    return 0

y = []

for i in X:
  y.append(binary_step(i))
  
plt.scatter(X,y)
plt.plot(X,y)
plt.show()


-------------------------------------------------------------------------------------------------------------------

#bipolar step
import numpy as np
import matplotlib.pyplot as plt

X = np.arange(-10,10)

def bipolar_step(x):
  th = 0.1
  if x >= th:
    return 1
  else:
    return -1

y = []

for i in X:
  y.append(bipolar_step(i))
  
plt.scatter(X,y)
plt.plot(X,y)
plt.show()

-------------------------------------------------------------------------------------------------------------------

#sigmoid
import numpy as np
import matplotlib.pyplot as plt

def binary_sigmoid(X,lamda=1):
  return (1/(1+np.exp(-lamda * X)))

def bipolar_sigmoid(X,lamda=10):
  return (1-np.exp(-lamda*X)/(1+np.exp(lamda*X)))

X = np.arange(-5,5)
y = binary_sigmoid(X)

plt.scatter(X,y)
plt.plot(X,y)
plt.show()

X = np.arange(-5,5)
y = bipolar_sigmoid(X)

plt.scatter(X,y)
plt.plot(X,y)
plt.show()



-------------------------------------------------------------------------------------------------------------------

#mcCulloch pitts or
import numpy as np
import matplotlib.pyplot as plt

X = np.array([[0,0],[0,1],[1,0],[1,1]])
W = np.array([1,1])

def predict(x,w):
  sum = np.sum(x*w)
  if sum >= 1:  #xor -> == 1 , #and -> >= 2 , #not -> < 1 
    return 1
  else:
    return 0

for e in X:
  y = predict(e,W)
  y = np.arange(0,2)
  B = 2-y
  plt.scatter(y,B)
  plt.plot(y,B)
  plt.show()


-------------------------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------------------------